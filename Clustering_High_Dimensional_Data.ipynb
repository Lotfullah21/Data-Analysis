{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering High Dimensional Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6l4iEKNXFEugCOUZjmJr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Data-Analysis/blob/main/Clustering_High_Dimensional_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsupervised Learning\n",
        "The goal of supervised learning is clear - we wish to learn a classifier which will predict the labels of future examples as accurately as possible. Furthermore, a supervised learner can estimate the success, or the risk, of its hypothesis using labeled training data by computing empirical loss, in contrast **clustering** is an unsupervised learning problem, namely there are no labels that we try to predict. instead we wish to organize the data in some meaningful way. as a result there is no clear success evaluation procedure for clustering"
      ],
      "metadata": {
        "id": "wj9UBZ8qDTCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "Clustering is one of the most widely used techniques for exploratory data analysis, from social science to biology to computer science, people try to get a first intuition about their data by identifying meaningful groups among the data points.\n",
        "Clustering is the task of grouping a set of objects such that similar objects end up in the same group and dissimilar objects are seperated into different groups, in other words, clustering aims to group of data points together so that within the same group, the observations are very similar and between different groups, the observations are dissimilar.\n",
        "Clustering is used as a way to decide what and how many class labels are suitable for the dataset.\n"
      ],
      "metadata": {
        "id": "d7FH7bcH5a_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematically speaking, similarity (or proximity) is not a transitive relation, while cluster sharing is an equivalence relation and in particular, it is transitive relation"
      ],
      "metadata": {
        "id": "ztZTRfTp289O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering Model:\n",
        "Clustering tasks can vary in terms of both the type of input they have and the type of outcome they are expected to compute.\n",
        "\n",
        "1. Input- a set of elements, X , and a distance function over it.\n",
        "2. Output- a partition of the domain set X into subsets\n",
        "\n",
        "We need to employ a stopping criteria inorder to do clustering, here we will mention some common of them.\n",
        "1. fixed number of clusters - fix some parameters, k , and stop merging as soon as the number of cluster is k.\n",
        "2. Distance upper bound - fix some r. stop merging as soon as all the between clusters distance are larger than r. \n"
      ],
      "metadata": {
        "id": "LU2MctWn4Kv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C19ORzQ1ZW4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GpK0ruQsZSHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application:\n",
        "for labeled datasets, classification algorithm such as Logistic Regression, SVMs, Random Forest Classifiers are well suited, but for datasets without their labels, the above mentioned algorithms will not able to give a satisfactory solution for us, Hence, the Clustering algorithms Comes into picture, it make use of all the features.\n",
        "\n",
        "Here are few areas which clustering is widely used.\n",
        "\n",
        "1. Customer Segmentation: we can cluster our customers based on their preferences, recent purchases, it can help us to know our customers more, so that we can serve the right things to the correct customers.\n",
        "2. For anamoly detection: we have the outliers which is not close by to any other datasets, it might be a someone who is having weird behavior in our system, maybe a thief.\n",
        "3. For data analysis: we can visualize our data and get some intuition about it by observing the clusters"
      ],
      "metadata": {
        "id": "7V-h-yHD7LSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-MEANS AND OTHER COST MINIMIZATION CLUSTERINGS\n",
        "Another possible approach to clustering starts by defining a cost function over parameterized set of possible clusterins and the goal of the clustering algorithm is to find partitioning(clustering) of minimal cost. Under this paradigm, the clustering task is turned into optimization problem.\n",
        "\n",
        "#### The K-means objective function: \n",
        "it is one of the most popular clustering objectives. In the k-means the data is partitioned into disjoints sets C1,...,Ck where each **Ci** represented by a **(Meu)i**.\n",
        "Here we want to cluster the data around a certain (Meu) , such that the distances between the data poins (Xs) are minimized. \n",
        "\n",
        "for example:\n",
        "in digital communication tasks, where the members of X may be viewed as a collection of signals that have to be transmitted, Now we have the transmission limits, that means we can transmit only a finite number of bits, one way to achieve this is to represent each X by a \"close\" member of some finit set of (Meu_1,Meu_2,Meu_3,...,Meu_k) and replace the transmission of any (x in X) by transmitting the idnex of the closest (Meu_k)\n"
      ],
      "metadata": {
        "id": "vDxhD5pWbMy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c5YQqe7b4HUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The K_Means Algorithm\n",
        "we are considering the problem of identifying groups, or clusters, of data points in a multidimensional space, suppose we have a data set {x_1,x_2,x_3,...,x_N} consisting of N observations of a random D-dimensional Euclidian variable x. Our goal is to partition the data set into some number K of clusters, where we shall suppose for comprising a group of data points whose inter-point distances are samll compared with the distance outside of the cluster.\n",
        "\n",
        "For each data point xn, we introduce a corresponding set of binary indicator variables (rnk in {0,1}), where K =1,2,3,...,K describing which of cluster k then rnk = , and rnj = 0, for j!=k.\n",
        "\n",
        "K(Fixed) clusters are obtained by minimizing some loss function, the loss function can be within group sum of squares(WGSS), the loss function can be optimized by assigining data point close to each other to the same cluster or also by maximizing the dissimilarities between sub groups.\n",
        "\n",
        "We are finding the K_Spheres in such a way that the distances are minimized from the points to the sphere radius.\n",
        "Exact solution in infeasible, because\n",
        "*   if we have N samples, K clusters, then: K^N possible assignment, impossible to search for all assignment.\n",
        "\n",
        "What to Do:\n",
        "1. Use Greedy algorithm\n",
        "2. Use random restart to avoid local optima.\n",
        "\n",
        "we are searching for means that distance are minimized between data and cluster mean.\n",
        "\n",
        "### **Algorithm**\n",
        "1. input: X in Rn; Number of cluster K\n",
        "2. initialize: Randomly choose initial centriods Meu_1,...,Meu_k\n",
        "3. repeat untill convergence\n",
        "\n",
        "Note:\n",
        "by convergence we mean untill the centroids are not changing any more\n",
        "Rn means our input in real space.\n",
        "\n",
        "\n",
        "the K_means algorithm is based on the use of squared Euclidian distance as the measure of dissimilarity between a data point and a prototype vector.\n",
        "it limits the type of data variables that can be considered, but it also make the determination of the cluster means nonrobust to outliers.\n",
        "it means the algorithm is very sensitive with respect to outliers, some time we can give freedom for outliers to cluster their own, but if we have for example 100 outliers, then we are having 100 clusters which is not desirable.\n",
        "\n",
        "**What to do ?**\n",
        "### K_Mediods: \n",
        "we partition our data set based on Mediods(PAM)\n",
        "it requires the cluster centriods to be member of the input set(one of the sample)\n",
        "The mediod of a cluster is the data point that is closest to the mean of that cluster.\n",
        "this method is having few advantages:\n",
        "1. More robust to outliers\n",
        "2. it gives representative object from each cluster\n",
        "3. it does not allow means to be any where in the space but in center(it can be one of sample)\n",
        "\n"
      ],
      "metadata": {
        "id": "k9k8VEcMjrQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aA6gMK-0xGN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}